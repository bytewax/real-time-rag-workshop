{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component, Document\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from haystack.dataclasses import ByteStream\n",
    "\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack import Pipeline\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "\n",
    "from haystack.utils import Secret\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "open_ai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def safe_deserialize(data):\n",
    "        \"\"\"\n",
    "        Safely deserialize JSON data, handling various formats.\n",
    "        \n",
    "        :param data: JSON data to deserialize.\n",
    "        :return: Deserialized data or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed_data = json.loads(data)\n",
    "            if isinstance(parsed_data, list):\n",
    "                if len(parsed_data) == 2 and (parsed_data[0] is None or isinstance(parsed_data[0], str)):\n",
    "                    event = parsed_data[1]\n",
    "                else:\n",
    "                    logger.info(f\"Skipping unexpected list format: {data}\")\n",
    "                    return None\n",
    "            elif isinstance(parsed_data, dict):\n",
    "                event = parsed_data\n",
    "            else:\n",
    "                logger.info(f\"Skipping unexpected data type: {data}\")\n",
    "                return None\n",
    "            \n",
    "            if 'link' in event:\n",
    "                event['url'] = event.pop('link')\n",
    "            \n",
    "            if \"url\" in event:\n",
    "                return event\n",
    "            else:\n",
    "                logger.info(f\"Missing 'url' key in data: {data}\")\n",
    "                return None\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"JSON decode error ({e}) for data: {data}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing data ({e}): {data}\")\n",
    "            return None\n",
    "        \n",
    "@component\n",
    "class BenzingaNews:\n",
    "    \n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, sources: Dict[str, Any]) -> None:\n",
    "             \n",
    "        documents = []\n",
    "        for source in sources:\n",
    "        \n",
    "            for key in source:\n",
    "                if type(source[key]) == str:\n",
    "                    source[key] = self.clean_text(source[key])\n",
    "                    \n",
    "            if source['content'] == \"\":\n",
    "                continue\n",
    "\n",
    "            #drop content from source dictionary\n",
    "            content = source['content']\n",
    "            document = Document(content=content, meta=source) \n",
    "            \n",
    "            documents.append(document)\n",
    "         \n",
    "        return {'documents':documents}\n",
    "               \n",
    "    def clean_text(self, text):\n",
    "        # Remove HTML tags using BeautifulSoup\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "file_path = \"./data/news_out.jsonl\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        news_event = safe_deserialize(line.strip())\n",
    "        results.append(news_event)\n",
    "        if not news_event:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BenzingaNews().run(sources=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_news = BenzingaNews()\n",
    "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "document_cleaner = DocumentCleaner(\n",
    "                    remove_empty_lines=True,\n",
    "                    remove_extra_whitespaces=True,\n",
    "                    remove_repeated_substrings=False\n",
    "                )\n",
    "document_splitter = DocumentSplitter(split_by=\"word\", split_length=5)\n",
    "document_writer = DocumentWriter(document_store=document_store,\n",
    "                                policy = DuplicatePolicy.OVERWRITE)\n",
    "embedding = OpenAIDocumentEmbedder(api_key=Secret.from_token(open_ai_key))\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"get_news\", get_news)\n",
    "pipeline.add_component(\"document_cleaner\", document_cleaner)\n",
    "pipeline.add_component(\"document_splitter\", document_splitter)\n",
    "pipeline.add_component(\"embedding\", embedding)\n",
    "pipeline.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "pipeline.connect(\"get_news\", \"document_cleaner\")\n",
    "pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "pipeline.connect(\"document_splitter\", \"embedding\")\n",
    "pipeline.connect(\"embedding\", \"document_writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(data={\"get_news\":{\"sources\":results}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
